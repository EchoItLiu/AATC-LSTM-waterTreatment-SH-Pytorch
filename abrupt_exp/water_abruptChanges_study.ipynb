{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abrupt Changes on Auto-Adjustable and Time-Consistent Long Short-Term Memory (AATC-LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn \n",
    "from DataSet import DataSet\n",
    "\n",
    "# AALSTM\n",
    "from models import AALSTM\n",
    "\n",
    "# NN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import config\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Water:\n",
    "    def __init__(self, excel_files):\n",
    "        self.source = DataSet(excel_files)\n",
    "        \n",
    "      # AATC-LSTM_AC: AATC-LSTM IS TRAINED WITH ABRUPT CHANGE DATASET\n",
    "      # AATC-LSTM_no_AC: AATC-LSTM IS TRAINED WITH DATASET HAS NO ABRUPT CHANGE \n",
    "      # NN_AC: A NEURAL NETWORK IS TRAINED WITH ABRUPT CHANGE DATASET\n",
    "      # NN_no_AC: A NEURAL NETWORK IS TRAINED WITH DATASET HAS NO ABRUPT CHANGE\n",
    "      \n",
    "    \n",
    "    def build_model(self, method, source):\n",
    "        \n",
    "        if method == 'AATC-LSTM_AC':\n",
    "            \n",
    "            aatc_ac_metric = self.lstmTraining(source)            \n",
    "            return aatc_ac_metric           \n",
    "                \n",
    "        if method == 'AATC-LSTM_no_AC':\n",
    "            \n",
    "            aatc_no_ac_metric = self.lstmTraining(source)\n",
    "            return aatc_no_ac_metric\n",
    "        \n",
    "        if method=='NN_AC':\n",
    "            \n",
    "            model = Sequential()\n",
    "            model.add(Dense(512, activation='relu', input_dim=m))\n",
    "            model.add(Dense(1))\n",
    "            adam = Adam(lr=1e-4)\n",
    "            model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "            model.fit(self.source.nn_trainX, self.source.nn_trainY, \n",
    "                                              epochs = 200, batch_size = 128,\n",
    "                                              verbose = 0\n",
    "                                             )\n",
    "            \n",
    "            pred_y = model.predict(source.nn_testX)\n",
    "            nn_ac_metric = self.evaluate(pred_y, self.source.nn_testY, metrics)[:2]         \n",
    "\n",
    "            return nn_ac_metric\n",
    "                \n",
    "        if method=='NN_no_AC':\n",
    "            \n",
    "            model = Sequential()\n",
    "            model.add(Dense(512, activation='relu', input_dim=m))\n",
    "            model.add(Dense(1))\n",
    "            adam = Adam(lr=1e-4)\n",
    "            model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "            model.fit(self.source.nn_trainX, self.source.nn_trainY, \n",
    "                                              epochs = 200, batch_size = 128,\n",
    "                                              verbose = 0\n",
    "                                             )\n",
    "            \n",
    "            pred_y = model.predict(source.nn_testX)\n",
    "            nn_no_ac_metric = self.evaluate(pred_y, self.source.nn_testY, metrics)[:2]         \n",
    "\n",
    "            return nn_no_ac_metric\n",
    "        \n",
    "    \n",
    "    def lstmTraining(self, source):\n",
    "        \n",
    "        valX = source.dagger_x_test\n",
    "        valY = source.dagger_y_test           \n",
    "        val_dagger_save_metric = []\n",
    "        aatc_metric = np.zeros([4])\n",
    "        self.bestScore_dagger = 1000          \n",
    "        self.in_features = valX.size(2)\n",
    "        self.epoch = 100\n",
    "        self.batchSize_train = 128\n",
    "        self.batchSize_val = 16\n",
    "        self.trainSeq =  self.seq_abrupt = source.deep_dagger_x.size(1)\n",
    "        self.testSeq = valX.size(1)\n",
    "\n",
    "        mse_criterion = nn.MSELoss()       \n",
    "        optimizer_dagger = torch.optim.Adam(aalstm.parameters(), lr = 1e-2)\n",
    "\n",
    "        loss_aatc_save = []\n",
    "        loss_dag_sum = 0\n",
    "        count_dag = 0\n",
    "\n",
    "        for i in range(self.epoch):\n",
    "\n",
    "            be_idx = 0\n",
    "\n",
    "            if i==0:\n",
    "                trainX_dag = source.deep_dagger_x\n",
    "                trainY_dag = source.deep_dagger_y\n",
    "            else:\n",
    "                trainX_dag = source.dagger_shuffle[:,:,:-1]\n",
    "                trainY_dag = source.dagger_shuffle[:,:,-1]\n",
    "\n",
    "            while(be_idx + self.batchSize_train) <= (trainX_dag.size(0) - 1):\n",
    "                optimizer_dagger.zero_grad()\n",
    "                \n",
    "                ed_idx = be_idx + self.batchSize_train                \n",
    "                out = aalstm(trainX_dag[be_idx:ed_idx,:,:], trainY_dag[be_idx:ed_idx,:], True)\n",
    "                   \n",
    "                AD_rate = cal_AC(trainX_dag[be_idx:(ed_idx + 1),:,:], self.batch_size_train, self.seq_train, 'batch')         \n",
    "                dag_params1 = aalstm.parameters()\n",
    "                L1_dag_sum = self.sum_params4L(dag_params1, 'L1')\n",
    "                dag_params2 = aalstm.parameters()\n",
    "                L2_dag_sum = self.sum_params4L(dag_params2, 'L2')\n",
    "\n",
    "\n",
    "                # Total Loss (TC Term)\n",
    "                loss_dag = AD_rate * mse_criterion(out, trainY_dag[be_idx:ed_idx,:]) + lambda1[1] * L1_dag_sum \\\n",
    "                + lambda2[2] * L2_dag_sum\n",
    "\n",
    "                loss_dag.backward()\n",
    "                optimizer_dagger.step()\n",
    "                loss_dag_sum = loss_dag + loss_dag_sum                    \n",
    "                be_idx = ed_idx\n",
    "                count_dag = count_dag + 1\n",
    "\n",
    "            average_dag_loss = loss_dag_sum / count_dag\n",
    "            epoch_num = 'epoch' + str(i)\n",
    "            loss_aatc_save.append(average_dag_loss)\n",
    "\n",
    "            loss_dag_sum = 0\n",
    "            count_dag = 0\n",
    "\n",
    "\n",
    "            ## validation\n",
    "            if i >= 5 and i%5==0:\n",
    "\n",
    "                batch_dag_val_b = 0\n",
    "                rmse_dag_score_total = 0\n",
    "                mape_dag_score_total = 0\n",
    "                corr_dag_score_total = 0\n",
    "                wi_dag_score_total = 0\n",
    "                count_dag_val = 0\n",
    "\n",
    "                val_oneBatch_dagger_metric = np.zeros([4])\n",
    "\n",
    "                while (batch_dag_val_b + self.batchSize_val) <= valX.size(0):\n",
    "                    with torch.no_grad():\n",
    "                        batch_dag_val_e = batch_dag_val_b + self.batchSize_val\n",
    "                        valXSlice = valX[batch_dag_val_b:batch_dag_val_e,:,:]\n",
    "                        valYSlice = valY[batch_dag_val_b:batch_dag_val_e]\n",
    "\n",
    "                        val_dag_out = daggertenlar(valXSlice, valYSlice, False)\n",
    "\n",
    "                        # calculate metrics for deep\n",
    "                        rmse_dag_score = self.cal_rmse4deep(val_dag_out, valYSlice, 'rmse')\n",
    "                        mape_dag_score = self.cal_rmse4deep(val_dag_out, valYSlice, 'mape')\n",
    "                        corr_dag_score = self.cal_rmse4deep(val_dag_out, valYSlice, 'correlation')\n",
    "                        wi_dag_score = self.cal_rmse4deep(val_dag_out, valYSlice, 'wi')\n",
    "\n",
    "                        rmse_dag_score_total = rmse_dag_score_total + rmse_dag_score\n",
    "                        mape_dag_score_total = mape_dag_score_total + mape_dag_score\n",
    "                        corr_dag_score_total = corr_dag_score_total + corr_dag_score\n",
    "                        wi_dag_score_total = wi_dag_score_total + wi_dag_score\n",
    "\n",
    "                        batch_dag_val_b = batch_dag_val_e\n",
    "                        count_dag_val = count_dag_val + 1\n",
    "\n",
    "                #     \n",
    "                val_oneBatch_dagger_metric[0] = rmse_dag_av_score = rmse_dag_score_total / count_dag_val\n",
    "                val_oneBatch_dagger_metric[1] = mape_dag_av_score = mape_dag_score_total / count_dag_val\n",
    "                val_oneBatch_dagger_metric[2] = corr_dag_av_score = corr_dag_score_total / count_dag_val\n",
    "                val_oneBatch_dagger_metric[3] = wi_dag_av_score = wi_dag_score_total / count_dag_val                       \n",
    "\n",
    "    #     \n",
    "\n",
    "                if rmse_dag_av_score < self.bestScore_dagger:\n",
    "                    print ('validation_dagger_best_score:', rmse_dag_av_score)\n",
    "                    print ('current_best_epoch:', i)\n",
    "\n",
    "                    val_dagger_save_metric.append(val_oneBatch_dagger_metric)\n",
    "                    self.bestScore_dagger = rmse_dag_av_score\n",
    "        # \n",
    "        val_save_metric_dagger = np.concatenate(val_dagger_save_metric)\n",
    "        val_save_metric_dagger = val_save_metric_dagger.reshape(-1, 4)\n",
    "\n",
    "        for i in range(val_save_metric_dagger.shape[1]):\n",
    "            val_save_metric_dagger_slice = val_save_metric_dagger[:, i]\n",
    "            val_save_metric_dagger_slice.sort(0)\n",
    "\n",
    "            if i==0 or i==1:\n",
    "                aatc_metric[i] = val_save_metric_dagger_slice[0][i]\n",
    "            else:\n",
    "                aatc_metric[i] = val_save_metric_dagger_slice[-1][i]\n",
    "                \n",
    "         # selecting RMSE and MAPE\n",
    "         aatc_metric = aatc_metric[:2]\n",
    "                       \n",
    "        return aatc_metric   \n",
    "\n",
    "    \n",
    "    def evaluate(self, pred_y, true_y, metrics):\n",
    "        pred_y = pred_y.reshape([-1])\n",
    "        true_y = true_y.reshape([-1])\n",
    "        metrics_num = len(metrics)\n",
    "        results = np.zeros([metrics_num])\n",
    "        for i,metric in enumerate(metrics):\n",
    "            if metric == 'rmse':\n",
    "                results[i] = np.sqrt(np.mean(np.power(pred_y - true_y,2)))\n",
    "            elif metric == 'mape':\n",
    "                results[i] = np.mean(np.abs(true_y - pred_y) / true_y)*100\n",
    "            elif metric == 'correlation':\n",
    "                results[i] = np.corrcoef(pred_y, true_y)[0,1]\n",
    "            elif metric == 'wi':\n",
    "                results[i] = np.sum(np.power(true_y - pred_y, 2)) / np.sum(np.power(np.abs(pred_y - np.mean(true_y)) + np.abs(true_y-np.mean(true_y)), 2))\n",
    "        return results      \n",
    "          \n",
    "        \n",
    "    def run_experiments(self, methods, experiments, repeat_num, metrics):\n",
    "        # dic type\n",
    "        results = {}\n",
    "        for exp_name in experiments.keys():\n",
    "            # exp_name is dict_keys type\n",
    "            exp = experiments[exp_name]\n",
    "            train_time, test_time = exp['times']\n",
    "            evaluation = np.zeros([repeat_num, len(exp['lag_time']), len(exp['clustering']), len(exp['normalize']), len(methods), (len(metrics)-2)])\n",
    "            print ('repeat_num:',repeat_num)\n",
    "                        \n",
    "            for r in range(repeat_num):\n",
    "                for i,lag_time in enumerate(exp['lag_time']):\n",
    "                    for j,clustering in enumerate(exp['clustering']):\n",
    "                        for k,normalize in enumerate(exp['normalize']):\n",
    "                            \n",
    "                                   \n",
    "                            # print and saving all ablative models\n",
    "                            for t, method in enumerate(methods):\n",
    "                                  \n",
    "                                if method=='AATC-LSTM_no_AC' or method=='NN_no_AC':\n",
    "                                \n",
    "                                    \"\"\"remove turbidy values have abrupt changes \"\"\"\" \n",
    "                                    self.source.attr2source(train_time, \n",
    "                                                            normalize,\n",
    "                                                            0, \n",
    "                                                            lag_time,\n",
    "                                                            data_type = 'NO_AC')                                                                 \n",
    " \n",
    "                                self.source.attr2source(train_time, \n",
    "                                                        normalize,\n",
    "                                                        0, \n",
    "                                                        lag_time,\n",
    "                                                        data_type = 'time-series')                            \n",
    "\n",
    "                                \n",
    "                                self.source.attr2source(train_time, \n",
    "                                                        normalize,\n",
    "                                                        clustering, \n",
    "                                                        lag_time,\n",
    "                                                        data_type = 'ts-train')\n",
    "            \n",
    "                                self.source.attr2source(test_time,\n",
    "                                                         normalize, \n",
    "                                                         clustering, \n",
    "                                                         lag_time,\n",
    "                                                         data_type = 'ts-test')\n",
    "                                                \n",
    "                                evaluation[r, i, j, k, t, :] = self.build_model(method, self.source) \n",
    "               \n",
    "                                print('exp_name={}, lag_time={}, normalize={}, clustering={}, method={} :{}'.format(\n",
    "                                    exp_name, lag_time, normalize, clustering, method, evaluation[r, i, j, k, t, :]))\n",
    "                                \n",
    "            # save results as a dictionary \n",
    "            np.save('result/{}.npy'.format(exp_name), evaluation)            \n",
    "\n",
    "\n",
    "    def decimal2round2(self, results):\n",
    "        sh = results.shape\n",
    "        fraction = results.reshape(-1)\n",
    "        for m in range(len(fraction)):\n",
    "            fraction[m] = round(fraction[m],8)\n",
    "        fractions = fraction.reshape(sh)\n",
    "#         print ('fractions_shape',fractions.shape)\n",
    "        return fractions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(config_final.path)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "m = Water(config_final.excel_files)\n",
    "m.run_experiments(config_final.methods, \n",
    "                  config_final.experiments,\n",
    "                  config_final.repeat_num,\n",
    "                  config_final.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating RMSE AND MAPE for Testing the Practicability of $TC$ Term in the AATC-LSTM to Abrupt Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AATC-LSTM_AC-RMSE-1.681-MAPE-3.079\n",
      "AATC-LSTM w/o AC-RMSE-1.594-MAPE-2.801\n",
      "NN_AC-RMSE-2.787-MAPE-5.765\n",
      "NN w/o AC-RMSE-2.1-MAPE-3.988\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# loading Spring as ablative dataset\n",
    "results_spring = np.load(osp.join('result', 'spring.npy'))\n",
    "result_spring  = results_spring.mean(0).squeeze()\n",
    "\n",
    "# type RMSE and MAPE\n",
    "RMSE_aatc_ac = result_spring[0][0] \n",
    "MAPE_aatc_ac = result_spring[0][1] \n",
    "\n",
    "RMSE_aatc_no_ac = result_spring[1][0] \n",
    "MAPE_aatc_no_ac = result_spring[1][1]\n",
    "\n",
    "RMSE_nn_ac = result_spring[2][0] \n",
    "MAPE_nn_ac = result_spring[2][1]\n",
    "\n",
    "RMSE_nn_no_ac = result_spring[3][0] \n",
    "MAPE_nn_no_ac = result_spring[3][1] \n",
    "\n",
    "\n",
    "print (\"AATC-LSTM_AC-RMSE-{}-MAPE-{}\".format(RMSE_aatc_ac, MAPE_aatc_ac))\n",
    "print (\"AATC-LSTM w/o AC-RMSE-{}-MAPE-{}\".format(RMSE_aatc_no_ac, MAPE_aatc_no_ac))\n",
    "print (\"NN_AC-RMSE-{}-MAPE-{}\".format(RMSE_aatc_ac, MAPE_aatc_ac))\n",
    "print (\"NN w/o AC:\".format(RMSE_aatc_ac, MAPE_aatc_ac))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
